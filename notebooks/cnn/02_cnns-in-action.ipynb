{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a54002f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:35.827789Z",
     "start_time": "2023-04-14T06:40:32.879347Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4795a06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:35.889103Z",
     "start_time": "2023-04-14T06:40:35.830917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53defe12",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in Action <a class='tocSkip'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cf4d3",
   "metadata": {},
   "source": [
    "*Machine Learning 3*\n",
    "\n",
    "*Prepared by Leodegario Lorenzo II*\n",
    "\n",
    "References:\n",
    "\n",
    "1. Deep Learning with PyTorch, Eli Stevents, Luca Antga, Thomas Viehmann (2022, Manning Publications)\n",
    "2. Deep Learning with Python, Francois Chollet, (2018, Manning Publications)\n",
    "3. Deep Learning for Coders with fastai and PyTorch, Jeremy Howard, Sylvain Gugger (2022, O'Reilly Media)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d42bd",
   "metadata": {},
   "source": [
    "## 0 PyTorch for deep learning <a class='tocSkip'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e311dc",
   "metadata": {},
   "source": [
    "**What is PyTorch?**\n",
    "\n",
    "PyTorch is a library for Python programs that facilitates building deep learning projects. It emphasizes flexibiity and allows deep learning models to be expressed in idiomatic Python.\n",
    "\n",
    "**Why PyTorch?**\n",
    "\n",
    "PyTorch is a suitable framework for deep learning projects for various reasons:\n",
    "\n",
    "1. It is simple. Users find it easy to learn, use, extend, and debug. It is not a framework allowing developers to be more expressive enabling them to implement complicated models without undue complexity.\n",
    "2. Deep learning is very natural in PyTorch. Its syntax is Pythonic, it feels very familiar to developers who have used Python previously.  It also provides facilities that support numerical optimizations on generic mathematical expressions which deep learning uses for training.\n",
    "3. Provides accelerated computation using GPUs and can be employed to use multiple machines during training. PyTorch was initially focused on research wor.kflows. Used C++ runtime that can be used to deploy models for inference without relying on Python.\n",
    "\n",
    "**What is the usual structure of a PyTorch project?**\n",
    "\n",
    "<img src='./images/pytorch-project.png' width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02d948",
   "metadata": {},
   "source": [
    "First, we need to physically get the dat most often from some storage. Then convert each sample to something that PyTorch can read: *tensors*. The training loop is usually implemented as a standard Python `for` loop. Afterwards, the model can be deployed on a server or export into a cloud engine or integrated in an application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e360a",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619f4e2",
   "metadata": {},
   "source": [
    "The dataset that we will use in our discussion is the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html). It is a dataset that consists of tiny 32 x 32 RGB images labeled with an integer corresponding to 1 of 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
    "\n",
    "We will use `torchvision` to download these datasets directly onto our local machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f8ee0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:36.251898Z",
     "start_time": "2023-04-14T06:40:35.894272Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d912864f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:38.300603Z",
     "start_time": "2023-04-14T06:40:36.255559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/cifar-10/'\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
    "               'horse', 'ship', 'truck']\n",
    "\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83900a1",
   "metadata": {},
   "source": [
    "We can inspect the datasets by directly indexing the `Dataset` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a80afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:38.830152Z",
     "start_time": "2023-04-14T06:40:38.303419Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78421703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:38.838686Z",
     "start_time": "2023-04-14T06:40:38.832596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32>, 1, 'automobile')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[99]\n",
    "img, label, class_names[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0976d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:38.924012Z",
     "start_time": "2023-04-14T06:40:38.841006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6b8aa",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6491ae",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc001fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:38.932109Z",
     "start_time": "2023-04-14T06:40:38.928175Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4c63a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:39.513858Z",
     "start_time": "2023-04-14T06:40:38.934664Z"
    }
   },
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878f04c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:47.304220Z",
     "start_time": "2023-04-14T06:40:39.516692Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296b217b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:47.338193Z",
     "start_time": "2023-04-14T06:40:47.306986Z"
    }
   },
   "outputs": [],
   "source": [
    "means = imgs.view(3, -1).mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e817cc94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:47.758015Z",
     "start_time": "2023-04-14T06:40:47.340293Z"
    }
   },
   "outputs": [],
   "source": [
    "stds = imgs.view(3, -1).std(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8733455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:47.768723Z",
     "start_time": "2023-04-14T06:40:47.761017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.49139965, 0.48215845, 0.44653094], dtype=float32),\n",
       " array([0.24703224, 0.24348514, 0.26158786], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9824496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:47.775255Z",
     "start_time": "2023-04-14T06:40:47.771498Z"
    }
   },
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "077d768a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:48.293556Z",
     "start_time": "2023-04-14T06:40:47.779141Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edf60bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:40:48.686258Z",
     "start_time": "2023-04-14T06:40:48.297643Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transformations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4a839",
   "metadata": {},
   "source": [
    "### Is it a bird? Is it a plane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d4fc4e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.168361Z",
     "start_time": "2023-04-14T06:40:48.688944Z"
    }
   },
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74624260",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc553703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.176454Z",
     "start_time": "2023-04-14T06:41:01.171328Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38e204",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed3649",
   "metadata": {},
   "source": [
    "<img src='./images/baseline-architecture.png' width=250/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6aa672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.187007Z",
     "start_time": "2023-04-14T06:41:01.178507Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b381a3a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.197863Z",
     "start_time": "2023-04-14T06:41:01.189508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8679af0",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2996ea63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.205412Z",
     "start_time": "2023-04-14T06:41:01.199404Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d94e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.209651Z",
     "start_time": "2023-04-14T06:41:01.207059Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "206af97c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:39:33.457236Z",
     "start_time": "2023-04-14T06:38:21.232605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-14 06:38:22.238977 Epoch 1, Training loss 0.579411323093305\n",
      "2023-04-14 06:38:28.994966 Epoch 10, Training loss 0.33024542536705165\n",
      "2023-04-14 06:38:35.793643 Epoch 20, Training loss 0.2915535759963807\n",
      "2023-04-14 06:38:42.423494 Epoch 30, Training loss 0.267597727809742\n",
      "2023-04-14 06:38:49.225015 Epoch 40, Training loss 0.24678098011738175\n",
      "2023-04-14 06:38:56.075038 Epoch 50, Training loss 0.2282219613623467\n",
      "2023-04-14 06:39:03.507108 Epoch 60, Training loss 0.2150772983670994\n",
      "2023-04-14 06:39:11.439463 Epoch 70, Training loss 0.19786105509016924\n",
      "2023-04-14 06:39:19.038775 Epoch 80, Training loss 0.18659346142582073\n",
      "2023-04-14 06:39:26.612989 Epoch 90, Training loss 0.16813478932069365\n",
      "2023-04-14 06:39:33.453152 Epoch 100, Training loss 0.15554255519987673\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(100, optimizer, model, loss_fn, train_loader=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821bf33",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bcfc518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:39:33.467131Z",
     "start_time": "2023-04-14T06:39:33.460214Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc7e6819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:39:33.925741Z",
     "start_time": "2023-04-14T06:39:33.469445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.92\n",
      "Accuracy val: 0.86\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416778f",
   "metadata": {},
   "source": [
    "## Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "798eb1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:39:33.935580Z",
     "start_time": "2023-04-14T06:39:33.928212Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './bird_or_plane.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57306cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:39:33.947139Z",
     "start_time": "2023-04-14T06:39:33.938080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load('./bird_or_plane.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453aaea",
   "metadata": {},
   "source": [
    "## Training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c126c84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:23:40.965737Z",
     "start_time": "2023-04-14T07:23:40.955229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7600eaed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T06:41:01.222061Z",
     "start_time": "2023-04-14T06:41:01.217483Z"
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f337abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:23:28.073544Z",
     "start_time": "2023-04-14T07:22:19.839338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-14 07:22:23.549917 Epoch 1, Training loss 0.5618082770876064\n",
      "2023-04-14 07:22:29.373536 Epoch 10, Training loss 0.3326062721431635\n",
      "2023-04-14 07:22:35.613281 Epoch 20, Training loss 0.29601489263735\n",
      "2023-04-14 07:22:42.293188 Epoch 30, Training loss 0.26567471093812567\n",
      "2023-04-14 07:22:47.487144 Epoch 40, Training loss 0.24442960160553076\n",
      "2023-04-14 07:22:54.177078 Epoch 50, Training loss 0.2269245014543746\n",
      "2023-04-14 07:23:00.747240 Epoch 60, Training loss 0.212961825500628\n",
      "2023-04-14 07:23:07.613348 Epoch 70, Training loss 0.19840166399813003\n",
      "2023-04-14 07:23:14.386494 Epoch 80, Training loss 0.18440728349860308\n",
      "2023-04-14 07:23:21.257854 Epoch 90, Training loss 0.17215606857352195\n",
      "2023-04-14 07:23:28.067509 Epoch 100, Training loss 0.15774712830212465\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(100, optimizer, model, loss_fn, train_loader=train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-msds2023-ml3-cnn]",
   "language": "python",
   "name": "conda-env-.conda-msds2023-ml3-cnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
